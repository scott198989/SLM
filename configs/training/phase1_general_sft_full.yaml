# Full-size HAVOC-7B training template (no optimizations tuned).
# Use after the smoke test passes. Adjust batch/accum/steps for your GPU(s).

model:
  vocab_size: 70000
  d_model: 4096
  num_layers: 32
  max_seq_len: 4096
  attention:
    num_heads: 32
    head_dim: 128
    num_kv_heads: 8
  mlp:
    hidden_dim: 11008

data:
  domain_ratio: 0.6
  general_ratio: 0.3
  dialog_ratio: 0.1
  max_sequence_length: 4096

tokenizer_path: artifacts/tokenizer
data_sources:
  - name: general_grounding
    type: jsonl
    paths:
      - data/general/havoc_general_grounding_6510.jsonl
      # Add more sources/globs when available:
      # - data/general/conversational_*.jsonl
      # - data/general/general_knowledge_*.jsonl
    weight: 1.0

batch_size: 1                        # per-device microbatch; adjust for VRAM
gradient_accumulation_steps: 64      # adjust to reach desired global batch
max_epochs: 1                        # use max_steps to control training length
max_steps: 50000                     # set to target steps for full run
learning_rate: 3.0e-4
weight_decay: 0.1
max_grad_norm: 1.0
warmup_steps: 2000
lr_scheduler_type: cosine
min_learning_rate: 3.0e-5

use_amp: true
amp_dtype: bfloat16

checkpoint_dir: /workspace/checkpoints    # ensure persistent mount
save_every_n_steps: 1000
keep_last_n_checkpoints: 3
resume_from_checkpoint: null

eval_every_n_steps: 1000
eval_samples: 50
log_eval_examples: 0
example_prompt_length: 64
example_max_new_tokens: 64

log_every_n_steps: 20
log_dir: /workspace/logs                  # ensure persistent mount
log_json_metrics: true
use_tensorboard: false

device: cuda
seed: 42
