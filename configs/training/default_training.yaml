# HAVOC-7B Default Training Configuration (RTX 5090 Optimized)

model:
  vocab_size: 70000
  d_model: 4096
  num_layers: 32
  max_seq_len: 2048         # ðŸ”» Reduced for VRAM
  dropout: 0.0
  layer_norm_eps: 1.0e-5
  initializer_range: 0.02
  pad_token_id: 0
  bos_token_id: 1
  eos_token_id: 2

  attention:
    num_heads: 32
    head_dim: 128
    num_kv_heads: 8
    rotary_dim: null
    rope_theta: 10000.0

  mlp:
    hidden_dim: 11008
    activation: swiglu

data:
  domain_ratio: 0.6
  general_ratio: 0.3
  dialog_ratio: 0.1
  max_sequence_length: 2048     # ðŸ”» Reduce activation memory

tokenizer_path: artifacts/tokenizer

data_sources:
  - name: general_grounding
    type: jsonl
    paths:
      - data/general/havoc_general_grounding_6510.jsonl
      - data/general/conversational_3500.jsonl
      - data/general/general_knowledge_5000.jsonl
    weight: 1.0

# === TRAINING ===  (Night Run â€“ RTX 5090 Optimized)

batch_size: 1                        # ðŸŸ© Safe for 32GB VRAM
gradient_accumulation_steps: 32      # ðŸŸ© Effective batch = 32

max_epochs: null                     # ðŸŸ¥ We use steps instead
max_steps: 10000                     # ðŸŸ© Let it cook all night

learning_rate: 3.0e-4
weight_decay: 0.1
max_grad_norm: 1.0

warmup_steps: 500                    # ðŸŸ© Better for long training
lr_scheduler_type: cosine
min_learning_rate: 3.0e-5

use_amp: true
amp_dtype: bfloat16

checkpoint_dir: checkpoints
save_every_n_steps: 500              # ðŸŸ© Save often enough not to lose shit
keep_last_n_checkpoints: 5
resume_from_checkpoint: null

eval_every_n_steps: 500
eval_samples: 256
log_eval_examples: 1
example_prompt_length: 128
example_max_new_tokens: 128

log_every_n_steps: 20
log_dir: logs
log_json_metrics: true
use_tensorboard: false

device: cuda
seed: 42
