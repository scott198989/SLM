# =============================================================================
# HAVOC Default Training Configuration
# =============================================================================
#
# CANONICAL SETTINGS - All vocab_size, token IDs, and hyperparameters
# are aligned with the tokenizer and model configs.
#
# This config is suitable for both 2B and 7B models with minor adjustments.
#
# =============================================================================

# -----------------------------------------------------------------------------
# MODEL ARCHITECTURE (2B default, override for 7B)
# -----------------------------------------------------------------------------
model:
  vocab_size: 70000           # CANONICAL: Must match tokenizer
  d_model: 2560
  num_layers: 20
  max_seq_len: 2048

  attention:
    num_heads: 32
    head_dim: null            # Will be d_model / num_heads
    num_kv_heads: 4           # GQA
    dropout: 0.0
    rotary_dim: null
    rope_theta: 10000.0

  mlp:
    hidden_dim: 10240
    activation: gelu

  # Regularization
  dropout: 0.0
  layer_norm_eps: 1.0e-5
  initializer_range: 0.02

  # Token IDs - MUST match tokenizer (pad=0, bos=1, eos=2, unk=3)
  pad_token_id: 0
  bos_token_id: 1
  eos_token_id: 2

# -----------------------------------------------------------------------------
# DATA CONFIGURATION
# -----------------------------------------------------------------------------
data:
  domain_ratio: 0.6
  general_ratio: 0.3
  dialog_ratio: 0.1
  max_sequence_length: 2048
  samples_per_epoch: 1024
  pack_sequences: true
  add_bos: true
  add_eos: true

tokenizer_path: /workspace/SLM/artifacts/tokenizer

data_sources:
  - name: pretrain
    type: text_directory
    paths:
      - /workspace/data/pretrain
    weight: 1.0

# -----------------------------------------------------------------------------
# TRAINING HYPERPARAMETERS - STABLE DEFAULTS
# -----------------------------------------------------------------------------

# Batch configuration
batch_size: 1
gradient_accumulation_steps: 8          # Effective batch = 8

# Training duration
max_epochs: null                        # Use max_steps
max_steps: 100000

# Optimizer - Conservative for from-scratch training
learning_rate: 1.5e-4                   # Conservative peak LR
weight_decay: 0.1                       # Standard
max_grad_norm: 1.0                      # Gradient clipping

# Adam optimizer settings
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-8

# Learning rate schedule
lr_scheduler_type: cosine               # Cosine annealing
min_learning_rate: 1.5e-5               # 10% of peak LR
warmup_steps: 2500                      # Long warmup for stability

# Mixed precision
use_amp: true
amp_dtype: bfloat16                     # bf16 preferred on modern GPUs

# -----------------------------------------------------------------------------
# CHECKPOINTING (300GB volume - rotation required)
# -----------------------------------------------------------------------------
checkpoint_dir: /workspace/SLM/checkpoints
save_every_n_steps: 250                 # Save every 250 steps
keep_last_n_checkpoints: 4              # Keep last 4 only (space constraint)
resume_from_checkpoint: null

# -----------------------------------------------------------------------------
# VALIDATION
# -----------------------------------------------------------------------------
eval_every_n_steps: 500
eval_samples: 100
log_eval_examples: 1
example_prompt_length: 64
example_max_new_tokens: 32

# -----------------------------------------------------------------------------
# LOGGING
# -----------------------------------------------------------------------------
log_every_n_steps: 10
log_dir: /workspace/SLM/logs
log_json_metrics: true
use_tensorboard: false

# -----------------------------------------------------------------------------
# DEVICE
# -----------------------------------------------------------------------------
device: cuda
seed: 42
