model:
  vocab_size: 70000
  d_model: 2560
  num_layers: 20
  max_seq_len: 1024

  attention:
    num_heads: 32
    num_kv_heads: 4
    dropout: 0.0

  mlp:
    hidden_dim: 10240
    activation: "gelu"

data:
  samples_per_epoch: 50000
  max_sequence_length: 1024
  pack_sequences: true
  add_bos: true
  add_eos: true

tokenizer_path: "/workspace/SLM/artifacts/tokenizer"

data_sources:
  - name: instruct_sft_clean
    type: jsonl
    paths:
      - /workspace/SLM/data/instruct_shuffled_text.jsonl
    weight: 1.0

batch_size: 1
gradient_accumulation_steps: 4

max_epochs: null
max_steps: 50000

learning_rate: 1e-5
weight_decay: 0.1
warmup_steps: 100
max_grad_norm: 1.0

lr_scheduler_type: "cosine"
min_learning_rate: 1e-6

use_amp: true
amp_dtype: "bfloat16"

checkpoint_dir: "/workspace/SLM/checkpoints/havoc_phase1_sft"
save_every_n_steps: 500
keep_last_n_checkpoints: 2

resume_from_checkpoint: "/workspace/SLM/checkpoints/havoc_academic_phase0/checkpoint_step_20000"

eval_every_n_steps: 500
eval_samples: 100
log_eval_examples: 1
example_prompt_length: 64
example_max_new_tokens: 64

log_every_n_steps: 10
log_dir: "/workspace/SLM/logs_phase1"
log_json_metrics: true
use_tensorboard: false

device: "cuda"
seed: 42
