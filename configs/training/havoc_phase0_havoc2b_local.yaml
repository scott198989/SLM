model:
  name: havoc-2b
  vocab_size: 70000
  d_model: 2560
  n_layers: 28
  n_heads: 32          # d_head = 2560 / 32 = 80
  d_ff: 10240
  max_seq_len: 2048
  dropout: 0.0
  rotary_embeddings: true
  weight_tying: true   # tie lm_head <-> embeddings if your code supports it
  norm_type: rmsnorm

data:
  train_path: data/HAVOC_PRETRAIN/train.txt
  val_path: data/HAVOC_PRETRAIN/validation.txt
  seq_len: 2048
  encoding: utf-8
  shuffle: true
  num_workers: 4       # or 2 if CPU is choking
  pack_sequences: true # if your loader supports packing

optimizer:
  name: adamw
  lr: 1.0e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]
  eps: 1.0e-8

scheduler:
  name: cosine
  warmup_steps: 1000
  max_steps: 100000    # adjust once you see speed

training:
  batch_size: 1
  grad_accumulation: 16      # effective batch size = 16
  max_steps: 5000            # start small for a smoke test, then ramp up
  log_interval: 50
  eval_interval: 500
  save_interval: 1000
  checkpoint_dir: checkpoints/havoc2b_phase0_local

hardware:
  device: cuda
  mixed_precision: bf16       # VERY IMPORTANT
  grad_checkpointing: true    # VERY IMPORTANT
  compile_model: false        # set true later if your code supports torch.compile

logging:
  project: havoc-pretrain
  run_name: havoc2b_local_phase0
