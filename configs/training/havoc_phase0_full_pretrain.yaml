# HAVOC Full Pretraining â€” REAL MODE
model:
  vocab_size: 70000
  d_model: 2560
  num_layers: 20
  max_seq_len: 2048
  dropout: 0.0
  layer_norm_eps: 1e-5
  initializer_range: 0.02
  pad_token_id: 0
  bos_token_id: 1
  eos_token_id: 2

  attention:
    num_heads: 32
    num_kv_heads: 4

  mlp:
    hidden_dim: 10240
    activation: gelu

tokenizer_path: /workspace/SLM/artifacts/tokenizer

data_sources:
  - name: havoc_pretrain_train
    type: jsonl
    paths:
      - /workspace/data/pretrain_train.jsonl
    weight: 0.98

  - name: havoc_pretrain_val
    type: jsonl
    paths:
      - /workspace/data/pretrain_val.jsonl
    weight: 0.02

data:
  max_sequence_length: 2048

batch_size: 1
gradient_accumulation_steps: 32   # Bigger effective batch for stability

max_steps: 500000
learning_rate: 3e-4
weight_decay: 0.1
warmup_steps: 2000
max_grad_norm: 1.0
lr_scheduler_type: cosine
min_learning_rate: 3e-5

use_amp: true
amp_dtype: bfloat16

checkpoint_dir: /workspace/SLM/checkpoints/havoc_pretrain_full
save_every_n_steps: 1000
keep_last_n_checkpoints: 5

eval_every_n_steps: 2000
eval_samples: 2048

log_every_n_steps: 50
log_dir: /workspace/SLM/logs/havoc_pretrain_full
