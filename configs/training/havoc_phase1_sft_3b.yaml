model: &havoc_model
  vocab_size: 70000
  d_model: 2560
  num_layers: 20
  max_seq_len: 1024
  attention:
    num_heads: 32
    head_dim: null
    num_kv_heads: 4
    dropout: 0.0
    rotary_dim: null
    rope_theta: 10000.0
    bias: false
  mlp:
    hidden_dim: 10240
    activation: swiglu
    dropout: 0.0
  dropout: 0.0
  layer_norm_eps: 1e-5
  initializer_range: 0.02
  pad_token_id: 0
  bos_token_id: 1
  eos_token_id: 2
model_config: *havoc_model

data_config:
  domain_ratio: 0.6
  general_ratio: 0.3
  dialog_ratio: 0.1
  max_sequence_length: 1024
  samples_per_epoch: 1024
  pack_sequences: true
  add_bos: true
  add_eos: true


data_sources:
  - type: jsonl
    paths:
      - sft_data/havoc_sft_phase1_full.jsonl
    weight: 1.0
    text_field: text


# ----------------------------
# TrainingConfig FIELDS (top level)
# ----------------------------

tokenizer_path: artifacts/tokenizer

checkpoint_dir: checkpoints/havoc_sft_phase1
resume_from_checkpoint: null

batch_size: 1
gradient_accumulation_steps: 4

max_epochs: null
max_steps: 1500

learning_rate: 1e-5
min_learning_rate: 1e-6
weight_decay: 0.1
warmup_steps: 50
max_grad_norm: 1.0

lr_scheduler_type: cosine

use_amp: true
amp_dtype: bfloat16

save_every_n_steps: 250
keep_last_n_checkpoints: 3

eval_every_n_steps: 200
eval_samples: 10
log_eval_examples: 1
example_prompt_length: 64
example_max_new_tokens: 32

log_every_n_steps: 20
log_json_metrics: true
use_tensorboard: false
log_dir: logs

device: cuda
seed: 42
