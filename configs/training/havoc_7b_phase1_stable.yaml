# =============================================================================
# HAVOC-7B Phase 1 Stable Training Configuration
# =============================================================================
#
# CANONICAL SETTINGS for 6-7B from-scratch pretraining.
# Conservative hyperparameters for stability.
#
# RUNPOD ENVIRONMENT:
#   - Hardware: AMD MI300X (ROCm) with bfloat16 AMP
#   - Repo root: /workspace/SLM
#   - Dataset root: /workspace/data
#   - Pretrain corpus: /workspace/data/pretrain
#   - Tokenizer: /workspace/SLM/artifacts/tokenizer
#   - Checkpoints: /workspace/SLM/checkpoints/havoc_7b_phase1
#   - Volume: 300GB (checkpoint rotation required)
#
# Usage:
#   python scripts/train.py --config configs/training/havoc_7b_phase1_stable.yaml
#
# =============================================================================

# -----------------------------------------------------------------------------
# MODEL ARCHITECTURE
# -----------------------------------------------------------------------------
model:
  vocab_size: 70000           # CANONICAL: Must match tokenizer
  d_model: 4096
  num_layers: 32
  max_seq_len: 4096

  attention:
    num_heads: 32
    num_kv_heads: 8           # GQA ratio 4:1
    head_dim: 128             # d_model / num_heads
    dropout: 0.0
    rotary_dim: 128
    rope_theta: 10000.0
    bias: false

  mlp:
    hidden_dim: 11008         # ~2.7x expansion for SwiGLU
    activation: swiglu
    dropout: 0.0

  # Regularization
  dropout: 0.0
  layer_norm_eps: 1.0e-5
  initializer_range: 0.02

  # Token IDs - MUST match tokenizer
  pad_token_id: 0
  bos_token_id: 1
  eos_token_id: 2

# -----------------------------------------------------------------------------
# DATA CONFIGURATION
# -----------------------------------------------------------------------------
data:
  domain_ratio: 0.6
  general_ratio: 0.3
  dialog_ratio: 0.1
  max_sequence_length: 4096
  samples_per_epoch: 10000
  pack_sequences: true
  add_bos: true
  add_eos: true

tokenizer_path: /workspace/SLM/artifacts/tokenizer

data_sources:
  - name: pretrain_corpus
    type: text_directory
    paths:
      - /workspace/data/pretrain
    weight: 1.0

# -----------------------------------------------------------------------------
# TRAINING HYPERPARAMETERS - STABLE PHASE 1 DEFAULTS
# -----------------------------------------------------------------------------

# Batch configuration
batch_size: 1                           # Per-device batch size
gradient_accumulation_steps: 8          # Effective batch = 8

# Training duration
max_epochs: null                        # Use max_steps for pretraining
max_steps: 100000                       # ~100k steps

# Optimizer - Conservative for 6-7B from-scratch
learning_rate: 1.5e-4                   # Conservative peak LR
weight_decay: 0.1                       # Standard
max_grad_norm: 1.0                      # Gradient clipping

# Adam optimizer settings
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-8

# Learning rate schedule
lr_scheduler_type: cosine               # Cosine annealing
min_learning_rate: 1.5e-5               # 10% of peak LR
warmup_steps: 2500                      # Long warmup for stability

# Mixed precision (AMD MI300X ROCm compatible)
use_amp: true
amp_dtype: bfloat16                     # REQUIRED for MI300X ROCm

# -----------------------------------------------------------------------------
# CHECKPOINTING (300GB volume - rotation required)
# -----------------------------------------------------------------------------
checkpoint_dir: /workspace/SLM/checkpoints/havoc_7b_phase1
save_every_n_steps: 250                 # Save every 250 steps
keep_last_n_checkpoints: 4              # Keep last 4 only (space constraint)
resume_from_checkpoint: null            # Set to checkpoint path to resume

# -----------------------------------------------------------------------------
# VALIDATION
# -----------------------------------------------------------------------------
eval_every_n_steps: 500
eval_samples: 100
log_eval_examples: 2
example_prompt_length: 128
example_max_new_tokens: 64

# -----------------------------------------------------------------------------
# LOGGING
# -----------------------------------------------------------------------------
log_every_n_steps: 10
log_dir: /workspace/SLM/logs/havoc_7b_phase1
log_json_metrics: true
use_tensorboard: false

# -----------------------------------------------------------------------------
# DEVICE
# -----------------------------------------------------------------------------
device: cuda
seed: 42
