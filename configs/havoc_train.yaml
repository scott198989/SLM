# -------------------------------
# HAVOC 3B TRAINING CONFIG
# -------------------------------

model:
  vocab_size: 70000
  d_model: 3072
  num_layers: 22
  max_seq_len: 2048

  attention:
    num_heads: 32
    num_kv_heads: 4

  mlp:
    hidden_dim: 12288

data:
  # Data mixture behavior (this is required!)
  domain_ratio: 0.6
  general_ratio: 0.3
  dialog_ratio: 0.1
  max_sequence_length: 2048
  samples_per_epoch: 1024
  pack_sequences: true
  add_bos: true
  add_eos: true

data_sources:
  - paths:
      - "/workspace/SLM/data/train.jsonl"
      - "/workspace/SLM/data/valid.jsonl"
    weight: 1.0

tokenizer_path: "/workspace/SLM/artifacts/tokenizer"

# --- Hyperparameters ---
batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 0.0003
weight_decay: 0.1
warmup_steps: 2000
max_steps: 20000
max_epochs: null
max_grad_norm: 1.0


# --- Scheduler ---
lr_scheduler_type: "cosine"
min_learning_rate: 0.00003

# --- Precision ---
use_amp: true
amp_dtype: "bfloat16"

# --- Checkpointing ---
checkpoint_dir: "/workspace/SLM/checkpoints"
save_every_n_steps: 1000
keep_last_n_checkpoints: 3
resume_from_checkpoint: null

# --- Eval ---
eval_every_n_steps: 500
eval_samples: 50
log_eval_examples: 1
example_prompt_length: 64
example_max_new_tokens: 32

# --- Logging ---
log_every_n_steps: 20
log_dir: "/workspace/SLM/logs"
log_json_metrics: true
use_tensorboard: false

# --- Device ---
device: "cuda"
seed: 42
